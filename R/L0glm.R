#' Fitting Constrained L0 Penalized Generalized Linear Models
#'
#' @import nnls
#' @import Matrix
#' @importFrom stats gaussian lm.fit pnorm poisson qnorm quantile rpois runif
#' @importFrom utils flush.console
#'
#' @description
#'
#' Fit a generalized linear model with nonnegativity contraints and ridge, adaptive ridge,
#' or best subset (L0) penalty on the coefficients. The function uses an iteratively reweighted
#' nonnegative least squares (IWNNLS) algorithm, which was adapted from the
#' regular IRLS algorithms to solve GLMs (McCullach & Nelder 1989).
#'
#' @param formula
#' a formula object describing the model to fit. See \code{\link{stats::formula}}
#' for more details.
#' @param data
#' the data from which to extract the covariates called in the formula. If \code{NULL},
#' data is retrieved from the environment associated with the formula.
#' @param weights a vector of observation weights ("prior weights") of length
#' \code{n}.
#' @param family
#' a description of the error distribution and link function to be
#' used in the model. This can be a character string naming a family function, a
#' family function or the result of a call to a family function. (See
#' \code{stats::family} for details of family functions.)
#' @param start
#' a vector of starting values for the coefficients to estimate.
#' Must be of length \code{p}.
#' @param contrasts
#' a list specifying the contrasts to use for the covariates when building the
#' design matrix. Default are the contrasts generated by \code{\link{contrasts}}.
#' @param lambda
#' either a scalar, a vector of numerics, or a character (see details). Set
#' \code{lambda = 0} to disable penalization.
#' @param no.pen
#' a vector of coefficient indices (intercept excluded) that should \strong{not}
#' be penalized.
#' @param nonnegative
#' a logical indicating whether nonnegativity constraints should be applied.
#' @param control.l0
#' list of parameters controling the L0 penalty loop (see details). The list
#' should contain the following elements:
#' \itemize{
#' \item{\code{maxit}: maximum number of iterations. Set to 1 for ridge penalty
#' (if \code{start == NULL}) or adaptive ridge penalty (if \code{start} != NULL).}
#' \item{\code{rel.tol}: coefficients are update until the relative difference
#' between two iteration is smaller than this threshold. The relative difference
#' is defined as
#' \eqn{2 \frac{|coef_i^{(k-1)} - coef_i^{(k)}|}{coef_i^{(k-1)} + coef_i^{(k)}}}}
#' \item{\code{delta}, \code{gamma}: parameters for computing the adaptive
#' weights during the penalization.}
#' \item{\code{warn}: should warnings from the AR iteration be produced ?}
#' }
#' Missing values will be filled with defaults (see \code{\link{control.l0.gen}})
#' @param control.iwls
#' list of parameters controling the IWLS loop (see details). The list should
#' contain the following elements:
#' \itemize{
#' \item{\code{maxit}: maximum number of iterations}
#' \item{\code{rel.tol}: coefficients are update until the relative difference
#' between two iteration is smaller than this threshold. The relative difference
#' is defined as
#' \eqn{2 \frac{|coef_i^{(k-1)} - coef_i^{(k)}|}{coef_i^{(k-1)} + coef_i^{(k)}}}}
#' \item{\code{thresh}: a threshold under which the fitted coefficients are
#' clipped to 0.}
#' \item{\code{warn}: should warnings from the IWLS iteration be produced ?}
#' }
#' Missing values will be filled with defaults (see \code{\link{control.iwls.gen}})
#' @param control.fit
#' list of parameters controling the core fitting algoritm (see details). The
#' list should contain the following elements:
#' \itemize{
#' \item{\code{maxit}: maximum number of iterations}
#' \item{\code{block.size}: number of covariates to fit simultaneously. If
#' \code{block.size < p}, the fit is performed in different blocks which is
#' updated cyclically. }
#' \item{\code{tol}: stop iteration when the difference in coefficients between
#' iterations is smaller than this threshold. The coefficient difference is
#' defined as:
#' \eqn{\frac{\|coefs^{(k-1)} - coefs^{(k)}}{\|coefs^{(k-1)}\|}}}
#' }
#' Missing values will be filled with defaults (see \code{\link{control.fit.gen}})
#' @param post.filter.fn
#' A function that performs post filtering to the coefficients at the end of the
#' fit. It should require a single argument that is the vector of coefficients.
#' Note that lambda tuning will be performed with the processed coefficients, so
#' the selected lambda will be influence by the filtering step. Default: no
#' filtering.
#' @param tune.meth
#' lambda selection method (see details). It should be one of \code{"none"},
#' \code{"IC"}, \code{"loocv"}, \code{"k-fold"} (where
#' k should be a numeric indicating the number of folds to use).
#' @param tune.crit
#' the criterion based on which to select the optimal lambda. Should be one of
#' \code{"all"}, \code{"bic"}, \code{"aic"}, \code{"loglik"}, \code{"loocv"},
#' \code{"rss"}, \code{"aicc"}, \code{"ebic"}, \code{"hq"}, \code{"ric"},
#' \code{"mric"}, \code{"cic"}, \code{"bicg"}, \code{"bicq"})
#' @param seed
#' a seed to use in case of \code{"k-fold"} CV.
#' @param verbose
#' print algorithm progression to console ?
#' @param converged_set
#' a vector of length \code{p} with logicals indicating whether the
#' corresponding coefficient already converged (\code{TRUE}) or not
#' (\code{FALSE}).
#'
#'
#' @details
#'
#' \strong{Structure of the algorithm}
#'
#' \code{L0glm} starts with lambda tuning (see below) and tries to select the
#' best lambda out of the serie of lambda supplied by the user, given a
#' tuning method and criterion. If a single lambda value is supplied, this
#' step is skipped. The best lambda is then used to fit the penalized GLM. The
#' algorithm contains 3 main nest loops. From high level to low level:
#' \itemize{
#' \item{\strong{Adaptive ridge for L0 penalty:} the L0 penalty is approximated
#' by iteratively fitting an adaptive ridge and update the penalty weights based
#' on the fitted coefficient from the previous update  (Frommlet et Nuel 2016).
#' The penalty weights are updated using the formula
#' \eqn{\frac{1}{(|\beta|^{\gamma} + \delta^{\gamma})}}.
#' When \code{control.l0$maxit == 1}, this boils down to a simple ridge penalty.
#' Furthermore, with \code{start != NULL}, an simple adaptive ridge is used where
#' \code{start} is considered as the coefficient priors.}
#' \item{\strong{GLM fitting using IWLS:} within every AR loop, a GLM is fit. If
#' the error family is Gaussian, this is a (nonnegative) least square regression.
#' For other error families and other link function than identity, the GLM is
#' solved using iteratively reweighted least squares (IWLS, McCullach & Nelder
#' 1989). To include ridge regularization, the covariate matrix is augmented
#' with the diagonal matrix \eqn{\sqrt{\lambda} \bm{I}}, and the response is
#' augmented with 0s accordingly.}
#' \item{\strong{Block-wise (NN)LS fitting:} TODO needed?}
#' }
#'
#' Information criteria and other goodness of fit of measures are computed based
#' on the fitted coefficients.
#'
#' \strong{Lambda tuning}
#'
#' Lambda can be specified in 3 ways:
#'
#'
#' \itemize{
#' \item{\strong{Scalar}: lambda is the penalty factor used for the ridge,
#' adaptive ridge, or best subset regression.}
#'
#' \item{\strong{Numerical vector}: every value of lambda will be used as penalty
#' factor for the ridge, adaptive ridge, or best subset regression, separately.
#' The lambda that leads to the best fit according to the provided selection
#' method and  criteria is kept and used to fit the model. If \code{tune.meth} is set
#' to \code{"none"}, the fit will be performed with every lambda and hence the
#' \code{"L0glm"} object will contain an \code{p x length(lambda)} matrix of
#' coefficients and \code{n x length(lambda)} matrix of fitted values.}
#'
#' \item{\strong{Character}}: In the case that L0 penalty is required (\emph{i.e.}
#' \code{control.l0$maxit > 1}), lambda can be supplied as the information
#' criterion to optimize. Possible values are
#' \code{"aic"} (\eqn{\lambda = 2}),
#' \code{"bic"} (\eqn{\lambda = ln(n)}),
#' \code{"hq"} (\eqn{\lambda = 2 ln(ln(n))}), or
#' \code{"bicq"} (\eqn{\lambda = ln(n) - 2 ln(\frac{q}{1-q})}, where q = 0.25).
#' }
#'
#' The lambda tuning can be performed using 3 different methods:
#' \itemize{
#' \item{When \code{tune.meth == "trainval"}, lambda is tuned using a training
#' and validation set. The training set will contain 90\% of the observations.
#' The best lambda is the lambda that leads to the highest performance according
#' to the IC measure computed on the validation set.}
#' \item{When \code{tune.meth == "IC"}, lambda is tuned on the full dataset. The
#' lambda associated with the best IC value will be selected. It is not
#' recommended using this method with \code{tune.crit == "rss"|"loglik"} since
#' it cannot overcome overfitting.}
#' \item{When \code{tune.meth == "k-fold"} (where \code{k} should be a numeric),
#' the lambda is tuned using k-fold cross-validation. The data is split in k
#' chunks where each chunk serves as a validation sets while the remaining data
#' serves as training set. The IC measure across folds are averaged. For LOOCV,
#' use \code{tune.meth == "IC"} and \code{tune.crit == "loocv"}.}
#' }
#'
#' To avoid unnecessary computations, the algorithm finds the maximal lambda for
#' which all coefficients are 0. This is given by
#' \deqn{\lambda_{max} = \max_{i = 1,\dots,p}~ 1/2  (X^\top_i X_i)^{-1} (X^\top_i y)^2}
#'
#' @return
#'
#' \code{L0glm} returns an "L0glm" object which contains the following
#' elements:
#' \item{\code{call}}{The call that produced this object.}
#' \item{\code{coefficients}}{a vector of fitted coefficients.}
#' \item{\code{constraint}}{the constrained used for fitting. Either "none" if
#' no constraint was applied or "nonneg" for nonnegative fits.}
#' \item{\code{converged.iwls}}{Did the last iteration of IWLS converged?}
#' \item{\code{converged.l0}}{Did the adaptive ridge converged?}
#' \item{\code{deviance}}{a list containing degrees of freedom and deviance
#' for the fitted model and the null model. The goodness of fit is computed as:
#' \eqn{D^2 = 1 - \frac{D_{res}}{D_{null}}}}
#' \item{\code{df}}{a list containing the \code{edf} effective degrees of freedom,
#' and \code{rdf} residual degree of freedom of the fitted model.}
#' \item{\code{eta}}{a vector with the fitted response on the link scale.}
#' \item{\code{family}}{The error structure used to fit the data.}
#' \item{\code{fitted.values}}{a vector with the fitted response.}
#' \item{\code{IC}}{a vector with IC values and other goodness of fit criteria.
#' Currently implemented are: \code{loglik}, \code{rss}, \code{aic}, \code{aicc},
#' \code{bic}, \code{ebic}, \code{hq}, \code{ric}, \code{mric}, \code{cic},
#' \code{bicg}, \code{bicq}.}
#' \item{\code{iter.iwls}}{A vector of length \code{iter.l0} containing the
#' number of iterations performed for every IWLS fit.}
#' \item{\code{iter.l0}}{The number of adaptive ridge iterations.}
#' \item{\code{lambda}}{The lambda used for the the adaptive ridge. This is a
#' vector of length \code{p} which may contain zeros for unpenalized coefficients.}
#' \item{\code{lambda.tune}}{a list generated during lambda tuning (if required).
#' The list contains the sequence of lambdas, the estimated coefficients for
#' every lambda (except if \code{tune.meth == "k-fold"}), the IC associated to
#' every lambda, and the selected lambda.}
#' \item{\code{lambda.w}}{The converged lambda weights from the adaptive ridge
#' iteration. This is a vector of length \code{p} with coefficient specific
#' weights.}
#' \item{\code{prior.coefficients}}{a vector with the supplied prior values of the
#' coefficients. NULL if no prior was supplied.}
#' \item{\code{prior.weights}}{a vector with the initial observation weights}
#' \item{\code{residuals}}{a vector of residuals.}
#' \item{\code{timing}}{time (in sec) taken for fitting the model (after lambda tuning).}
#' \item{\code{weights}}{a vector of converged IWNNLS weights. This is a vector
#' of length \code{n}.}
#'
#'
#' @note
#' When a lambda is supplied as vector with no tuning method (\code{tune.meth == "none"}),
#' the function returns an object of class \code{tune.L0glm}. This contains
#' \code{coefficients} (a \code{p} by \code{length(lambda)} matrix), \code{fitted.values}
#' (a \code{n} by \code{length(lambda)} matrix), \code{family}, and \code{prior.weights}.
#'
#' @author
#' Tom Wenseleers, Christophe Vanderaa
#'
#' @references
#'
#' Frommlet, F. and G., Nuel. (2016) An adaptive ridge procedure for L0 regularization. PloS one.
#'
#' Hastie, T. J. and Pregibon, D. (1992) Generalized linear models. Chapter 6 of Statistical Models in S eds J. M. Chambers and T. J. Hastie, Wadsworth & Brooks/Cole.
#'
#' McCullagh P. and Nelder, J. A. (1989) Generalized Linear Models. London: Chapman and Hall.
#'
#' @seealso
#' \code{\link{control.l0.gen}}, \code{\link{control.iwls.gen}},
#' \code{\link{control.fit.gen}}
#'
#' @example examples/L0glm_examples.R
#'
#' @export
L0glm <- function(formula,
                  data = NULL,
                  weights = rep(1,length(y)),
                  family = gaussian(identity),
                  start = NULL,
                  contrasts = NULL,
                  lambda = 0, no.pen = 0,
                  nonnegative = FALSE,
                  control.l0 = list(),
                  control.iwls = list(),
                  control.fit = list(),
                  post.filter.fn = function(u) return(u),
                  # TODO make CV arguments a single list, eg tune.control ?
                  tune.meth = "none",
                  tune.crit = "bic",
                  seed = NULL, # set seed for k-fold CV
                  # More params
                  verbose = TRUE){

  call0 <- match.call()

  # Extract data structure from the formula (code taken from stats::glm)
  if (is.null(data)) data <- environment(formula)
  mf <- match.call(expand.dots = FALSE)
  m <- match(c("formula", "data", "weights"), names(mf), 0L)
  mf <- mf[c(1L, m)]
  mf$drop.unused.levels <- TRUE
  mf[[1L]] <- quote(stats::model.frame)
  mf <- eval(mf, parent.frame())
  mt <- attr(mf, "terms")
  # Get the response variable
  y <- model.response(mf, "any")
  if (length(dim(y)) == 1L) {
    nm <- rownames(y)
    dim(y) <- NULL
    names(y) <- nm
  }
  # Get the design matrix
  X <- if (!is.empty.model(mt)){
    model.matrix(mt, mf, contrasts)
  } else {
    matrix(NA, length(y), 0L)
  }
  intercept <- as.logical(attr(mt, "intercept"))
  if(intercept) no.pen <- c(0, no.pen)

  # Get control parameters
  control.l0 <- do.call("control.l0.gen", control.l0)
  control.iwls <- do.call("control.iwls.gen", control.iwls)
  control.fit <- do.call("control.fit.gen", control.fit)

  n <- nrow(X) # nr of obersvations
  p <- ncol(X) # nr of covariates

  if(control.l0$warn && control.l0$delta > control.iwls$thresh){
    control.l0$delta <- control.iwls$thresh
    warning("control.l0$delta > control.iwls$thresh. To avoid artifacts control.l0$delta is decreased to control.iwls$thresh")
  }

  # Check family argument
  if(is.character(family)) family <- get(family, mode = "function", envir = parent.frame())
  if(is.function(family)) family <- family()
  if(family$family == "gaussian" && family$link == "identity") control.iwls$maxit <- 1 # with gaussian identity loss we end up doing single nnls iteration
  if(family$family == "poisson") y <- as.integer(y)

  # Get lambda with or without cross-validation
  if(any(lambda < 0)) stop("Negative lambda not allowed")
  if(control.l0$warn && sum(lambda) == 0 & control.l0$maxit > 1){
    control.l0$maxit <- 1
    warning("When no penalty is required (ie lambda == 0), there is no need for adaptive ridge iteration. Setting control.l0$maxit = 1.")
  }
  if(is.character(lambda)){
    if(control.l0$maxit == 1) stop("You cannot specify lambda as an information criterion without L0 penalty.")
    lambda <- switch(lambda,
                     aic = 2,
                     bic = log(n),
                     hq = 2*log(log(n)), # Hannan and Quinnn information criterion,
                     bicq = log(n) - 2*log(0.25/(1-0.25)), # TODO q = 0.25 should be a constant to optimize...
                     stop("invalid information criterion for initializing lambda."))
    tune.meth = "none"
    tune.crit = NA
  }
  if(tune.meth == "loocv") { # LOOCV is performed on the full data using the shortcut as described in https://scholarworks.gsu.edu/cgi/viewcontent.cgi?referer=https://scholar.google.com/&httpsredir=1&article=1100&context=math_theses
    tune.meth <- "IC"
    tune.crit <- "loocv"
  }
  tune.crit <- match.arg(tune.crit, c("all", "bic", "aic", "loglik", "loocv", "rss", "aicc", "ebic",
                                      "hq", "ric", "mric", "cic",  "bicg", "bicq"))

  if(verbose) cat(paste0("\n==== Lambda tuning ====\n\n",
                         "Number of lambda values: ", length(lambda), "\n",
                         "Lambda tuning method: ", tune.meth, "\n",
                         "Lambda tuning criterion: ", tune.crit, "\n",
                         "Tuning lambda...\n"))
  lambda.tune <- list()
  # Trim the lambda to the theoretical upper bound
  # see https://stats.stackexchange.com/questions/416144/minimum-and-maximum-regularization-in-l0-pseudonorm-penalized-regression/417080#417080
  lam.max <- 1/2 * max(1/diag(crossprod(X, X)) * crossprod(X, y)^2)
  if(control.l0$warn && any(lambda > lam.max)){
    warning(paste0("At least one lambda value was higher than the theoretical upper bound (lambda = ", lam.max, "). The values are removed from the sequence."))
    lambda <- lambda[lambda < lam.max]
    if(length(lambda) == 0) stop("No valid lambda supplied, try using lower lambda values.")
  }
  if(tune.meth == "none"){
    tune.crit <- NA
    if(length(lambda) > 1){
      fits <- lapply(lambda, function(l){
        l <- rep(l, p)
        l[no.pen] <- 0
        fit <- L0glm.fit(X = X, y = y, weights = weights, family = family,
                         lambda = l, start = start, nonnegative = nonnegative,
                         control.l0 = control.l0, control.iwls = control.iwls, control.fit = control.fit,
                         post.filter.fn = post.filter.fn)
      })
      fit <- list()
      fit$coefficients <- sapply(fits, "[[", "coefficients")
      fit$fitted.values <- sapply(fits, "[[", "fitted.values")
      colnames(fit$coefficients) <- colnames(fit$fitted.values) <- lambda
      fit$family <- family
      fit$prior.weights <- weights
      return(structure(fit, class = "tune.L0glm"))
    }
  } else if (tune.meth %in%c("trainval", "IC", "loocv") || grepl(pattern = "^\\d+-fold$", tune.meth)) {
    if(length(lambda)<2) stop("Need more than one lambda value for optimization.")
    if(tune.meth == "trainval"){
      lambda.tune <- L0glm.trainval(X, y, weights = weights, family = family,
                                    start = start, lambdas = lambda, no.pen = no.pen,
                                    nonnegative = nonnegative, control.l0 = control.l0,
                                    control.iwls = control.iwls, control.fit = control.fit,
                                    tune.crit = tune.crit, seed = seed,
                                    verbose = verbose)
    } else if(tune.meth == "IC"){
      lambda.tune <- L0glm.IC(X, y, weights = weights, family = family,
                              start = start, lambdas = lambda, no.pen = no.pen,
                              nonnegative = nonnegative, control.l0 = control.l0,
                              control.iwls = control.iwls, control.fit = control.fit,
                              tune.crit = tune.crit,
                              verbose = verbose)
    } else { # if tune.meth == "x-fold"
      k <- round(as.numeric(gsub(tune.meth, pattern = "-fold", replacement = "")))
      if(k < 2) stop("'tune.meth' should be at least 2-fold")
      lambda.tune <- L0glm.cv(X, y, weights = weights, family = family,
                              start = start, lambdas = lambda, no.pen = no.pen,
                              nonnegative = nonnegative, control.l0 = control.l0,
                              control.iwls = control.iwls, control.fit = control.fit,
                              tune.crit = tune.crit, k = k, seed = seed,
                              verbose = verbose)
    }
    lambda <- lambda.tune$best.lam
  } else {
    stop("'tune.meth' should be one of: none, trainval, IC, loocv, k-fold (where k should be a numeric indicating the number of folds to use)")
  }
  lambda0 <- lambda
  lambda <- rep(lambda, p)
  lambda[no.pen] <- 0

  if(verbose) cat(paste0("Tuned lambda value: ", lambda0, "\n\n"))


  # Fit the model
  if(verbose) cat(paste0("==== Model fitting ====\n\n",
                         "Fitting in progress...\n"))
  t1 <- proc.time()[3]
  fit <- L0glm.fit(X = X, y = y, weights = weights, family = family, lambda = lambda,
                   start = start, nonnegative = nonnegative, control.l0 = control.l0,
                   control.iwls = control.iwls, control.fit = control.fit,
                   post.filter.fn = post.filter.fn)
  fit$timing <- proc.time()[3] - t1
  if(verbose) cat(paste0("Converged: ", ifelse(fit$converged.l0, "yes", "no"), "\n",
                         "Number of adaptive ridge (L0 penalty) iterations: ", fit$iter.l0, "\n",
                         "Number of IWLS iterations: ", paste0(fit$iter.iwls, collapse = ", "), "\n\n"))

  nonzero <- fit$coefficients != 0
  fit$call <- call0
  fit$constraint <- ifelse(nonnegative, "nonnegative", "none")
  fit$prior.coefficients <- start
  fit$lambda.tune <- lambda.tune

  # Compute residuals
  fit$residuals <- y - fit$fitted.values

  # If all coefficient are zero, return fit without inference
  if(sum(nonzero) == 0){
    fit$deviance <- c(residual.deviance = NA, residual.df = NA,
                      null.deviance = NA, null.df = NA, D.squared = NA)
    fit$df <- c(edf = NA, rdf = NA)
    fit$IC <- compute.ic(y = y, y.hat = fit$fitted.values, w = fit$prior.weights, fit = fit)
    fit$coefficient.inference <- data.frame(estimates = fit$coefficients, se = NA, pvalues = NA, method = NA)
    if(control.l0$warn) warning("All coefficients are 0.")
    return(fit)
  }

  # Compute degrees of freedom
  fit$df <- compute.df(fit = fit, X = X)
  # Compute deviance
  int.ind <- grepl("intercept", colnames(X), ignore.case = TRUE)
  dev.res <- sum(fit$family$dev.resids(y = y, mu = fit$fitted.values, wt = fit$prior.weights))
  fit.null <- glm.iwls(X = X[,int.ind, drop = F],
                       y = y, weights = fit$prior.weights, family = family,
                       lambda = 0, start = NULL, nonnegative = nonnegative,
                       control.iwls = control.iwls, control.fit = control.fit)
  dev.null <- sum(family$dev.resids(y = y, mu = fit.null$fitted.values, wt = fit.null$prior.weights))
  if(is.infinite(dev.null)) dev.null <- NA
  fit$deviance <- list(residual.df = fit$df$rdf,
                       null.df = sum(fit.null$prior.weights != 0) - sum(int.ind),
                       residual.deviance = dev.res, # residual deviance = 2 * (LL_full - LL_model)
                       null.deviance = dev.null, # null deviance = total deviance = 2 * (LL-full - LL_null)
                       D.squared = 1 - dev.res/dev.null)

  # Compute the goodness of fit criteria
  fit$IC <- compute.ic(y = y, y.hat = fit$fitted.values, w = fit$prior.weights, fit = fit)

  # Compute R squared
  # R2 is computed on the adjusted scale using the optimal weights from the last
  # iteration of the IWLS.
  # TODO shall we include this ?
  # eta <- fit$eta # Sample R2 (the variance is estimated from the model fit of the last IRLS iteration)
  # # eta <- X %*% coefficients.true # Population R2 (true variance is known)
  # z <- eta + (y - fit$fitted.values)/family$mu.eta(eta) # = X %*% beta + (y - g.inv(mu))/g'(X %*% beta)
  # z.fit <- family$linkfun(fit$fitted.values) # Fitted values on the link scale
  # z.null <- family$linkfun(fit.null$fitted.values)
  # w <- weights * as.vector(family$mu.eta(eta)^2 / family$variance(family$linkinv(eta))) # IWLS weights from last iteration or from true model
  # mss <- sum(w * (z.fit - z.null)^2) # model sums of squares
  # tss <- sum(w * (z - z.null)^2) # total sums of squares
  # fit$R.squared <- list(R.squared = mss/tss, # = Model SS / Total SS
  #                       type = ifelse(is.null(coefficients.true), "sample", "population"))

  # Return solution
  fit <- fit[sort(names(fit))]
  return(structure(fit, class = "L0glm"))
}


#' @describeIn L0glm
#'
#' fits an L0 penalized regression using the adaptive ridge algorithm (Frommlet
#' et Nuel 2016). The function contains minimal code for fitting the adaptive
#' ridge and is called by the higher level \code{L0glm}.
#'
#' @export
L0glm.fit <- function(X, y,
                      weights = rep(1, length(y)),
                      family = poisson(identity),
                      lambda = 0,
                      start = NULL,
                      nonnegative = FALSE,
                      post.filter.fn = function(u) return(u),
                      control.l0 = list(maxit = 100, rel.tol = 1E-4, delta = 1E-5, gamma = 2, warn = FALSE),
                      control.iwls = list(maxit = 100, rel.tol = 1E-4, thresh = 1E-5, warn = FALSE),
                      control.fit = list(maxit = 10, block.size = NULL, tol = 1E-7)){
  n <- nrow(X)
  p <- ncol(X)
  if(length(lambda) == 1) lambda <- rep(lambda, p)
  y <- as.vector(y)

  # The adaptive ridge loop
  mask <- rep(FALSE, p)
  iter.iwls <- vector()
  beta <- if(is.null(start)) rep(1,p) else start

  for (i in 1:control.l0$maxit) {
    beta0 <- beta # coefs from previous iteration
    lambda_adap <- lambda * AR.weights(beta = beta, delta = control.l0$delta, gamma = control.l0$gamma) # ridge penalty with adaptive weights

    # Fit the model with current adaptive weights.
    fit <- glm.iwls(X = X, y = y, weights = weights, family = family, start = start,
                    lambda = lambda_adap, converged_set = mask, nonnegative = nonnegative,
                    control.iwls = control.iwls, control.fit = control.fit)
    iter.iwls[i] <- fit$iter.iwls
    beta <- start <- fit$coefficients # Note: beta and start is updated because this allows that if only 1 iteration is required, the loop boils down to a non penalized glm

    # Control iteration
    reldiff <- 2*abs((beta-beta0)/(beta+beta0))
    mask <- reldiff <= control.l0$rel.tol # keep coefficients fixed in subsequent iterations if they converged
    mask[is.na(mask)] <- TRUE
    stop.loop <- sum(mask) >= (p-1)

    # TODO Delete
    # plot(beta, type = "h", col = 2, main = i)
    # Sys.sleep(0.5)
    if(stop.loop) break
  }
  if(control.l0$warn && control.l0$maxit > 1 && !stop.loop)
    warning("Best subset algorithm did not converge, maybe increase control.l0$maxit")

  fit$iter.iwls <- iter.iwls
  fit$iter.l0 <- i
  fit$lambda <- lambda
  fit$lambda.w <- lambda_adap/lambda # Converged lambda weights
  fit$lambda.w[is.na(fit$lambda.w)] <- 0 # happens when dividing by 0
  fit$converged.l0 <- stop.loop || control.l0$maxit == 1

  # Perform post filtering of the coefficients using user supplied function
  fit$coefficients <- post.filter.fn(fit$coefficients)
  fit$post.filter.fn <- post.filter.fn
  return(fit)
}


#' @describeIn L0glm
#'
#' fits a GLM using iteratively weighted least squares (McCullagh and Nelder 1989). Note that
#' \code{lambda} should be a vector of length \code{p} and no lambda tuning is
#' performed.
#'
glm.iwls <- function(X, y, weights, family,
                     start = NULL,
                     lambda = rep(0, ncol(X)),
                     nonnegative = FALSE,
                     control.iwls = list(maxit = 100, rel.tol = 1E-4, thresh = 1E-5, warn = FALSE),
                     control.fit = list(maxit = 10, block.size = NULL, tol = 1E-7),
                     converged_set = rep(FALSE, ncol(X))){
  n <- nrow(X)
  p <- ncol(X)

  if (p == 0) { # IN case a NULL model is fit, this chunk is taken from 'glm'
    return(list(coefficients = numeric(),
                weights = numeric(),
                prior.weights = weights,
                fitted.values = rep(0, n),
                family = family,
                converged.iwls = TRUE,
                iter.iwls = 0))
  }

  # Row augmentation of the covariate matrix if lambda!=0 to add ridge penalty
  # Ref :
  # https://stats.stackexchange.com/a/164546/27340
  # https://stats.stackexchange.com/a/203699/27340
  if (sum(lambda) != 0){
    Xaug <- rbind(X, diag(sqrt(lambda),ncol(X)))
  } else {
    Xaug <- X
  }

  # Initialize coefficients
  beta <- if(!is.null(start)){
    start
  } else {
    rep(1,ncol(X))
  }
  names(beta) <- colnames(X)

  # Initialize fit
  if (!is.null(start)){
    eta <- X %*% beta
    mu <- family$linkinv(eta)
  } else {
    # Initial estimate
    nobs <- n # needed for evaluating the initialization
    etastart <- mustart <- NULL # needed for evaluating the initialization
    eval(family$initialize) # generates mustart
    mu <- mustart
    eta <- family$linkfun(mustart)
  }

  # Initialize iteration control variables
  niters_nochange <- converged_set * control.iwls$maxit # nr of iterations over which coefficients showed less than rel.tol relative change.
  nonzero <- rep(TRUE, ncol(X)) # nonzero coefficients
  for(j in 1:control.iwls$maxit) {

    # Update partial residuals (only for identity links)
    # We work on partial residuals yres, ie the model predictions minus the
    # predictions for covariates with nonzero coefficients that already
    # converged. This allows for a dramatic speed up after the 1st iteration.
    if(family$link != "identity") converged_set <- rep(FALSE, ncol(X)) # only works for identity link
    yres <- y - as.vector(X[, converged_set & nonzero, drop=F] %*% beta[converged_set & nonzero])

    # Linearize the response and adapt weights
    # The updates use Fisher scoring, which is Newton-Raphson with
    # the expected Hessian, see
    # https://stats.stackexchange.com/questions/344309/why-using-newtons-method-for-logistic-regression-optimization-is-called-iterati
    if(family$link == "identity"){ # simplified formula and faster than formula in else statement
      W <- W0 <- as.vector(weights/family$variance(mu)) # 1/variance (* weights) with identity link, where variance = 1 with Gaussian noise, variance = mu with Poisson noise, variance = mu^2 with Gamma noise
      z <- yres
    } else {
      mu.prime <- family$mu.eta(eta)
      z <- as.vector(eta + (yres - mu) / mu.prime) # TODO Remove offset here if needed.
      W <- W0 <- as.vector(weights * mu.prime^2 / family$variance(mu))
    }

    # Row augmentation of the updated weights and observations
    if(sum(lambda) != 0) {
      W <- c(W, rep(1,p))
      zaug <- c(z, rep(0,p))
    } else {
      zaug <- z
    }

    # Update the observation subset
    good <- is.finite(W) & W>0
    betaold <- beta

    # Weighted LS update, with optional nonnegativity constraints
    # TODO talk with Tom
    if(is.null(control.fit$block.size)) control.fit$block.size <- p
    block.ids <- ceiling((1:p)/control.fit$block.size)
    beta[!converged_set] <- block.fit(X = Xaug[good,!converged_set,drop=F]*sqrt(W[good]),
                                      y = zaug[good]*sqrt(W[good]),
                                      block.id = block.ids[!converged_set],
                                      nonnegative = nonnegative,
                                      control = control.fit)
    # TODO clean this
    # if(F){
    #   if(nonnegative){
    #     beta[!converged_set] <- nnls(A = Xaug[good,!converged_set,drop=F]*sqrt(W[good]),
    #                                  b = zaug[good]*sqrt(W[good]))$x
    #   } else {
    #     beta[!converged_set] <- lm.fit(x = as.matrix(Xaug[good,!converged_set,drop=F]*sqrt(W[good])),
    #                                    y = zaug[good]*sqrt(W[good]),
    #                                    tol = 1E-07)$coefficients
    #     beta[is.na(beta) | is.infinite(beta)] <- 0
    #   }
    # }

    beta[abs(beta) < control.iwls$thresh] <- 0
    nonzero <- beta != 0

    # Update fitted data
    eta <- X[,nonzero,drop=F] %*% beta[nonzero] # + offset if required
    mu <- family$linkinv(eta)

    # Update iteration control
    reldiff <- 2*abs((beta-betaold)/(beta+betaold)) # Vector of relative differences in coefficient estimates
    reldiff[is.na(reldiff)] <- 0
    niters_nochange <- niters_nochange + (reldiff <= control.iwls$rel.tol)*1
    # if coefficients stay within rel.tol in terms of relative difference over
    # 2 subsequent iterations we consider they converged. If initial estimates
    # are supplied, we look only at previous iteration
    # This is perfromed only when working with identity link
    converged_set <- niters_nochange >= (2 - !is.null(start))
    if (anyNA(beta)) {
      if(control.iwls$warn) warning("Error: fit failed - NaN coefficients produced")
      break
    }
    if(all(converged_set)) break
  }
  if(control.iwls$warn && !all(converged_set) && control.iwls$maxit > 1)
    warning("IWLS algorithm did not converge, maybe increase control.iwls$maxit")

  # Return output
  return(list(coefficients = beta,
              weights = W0,
              prior.weights = weights,
              fitted.values = as.vector(mu),
              eta = eta,
              family = family,
              converged.iwls = all(converged_set) || control.iwls$maxit == 1,
              iter.iwls = j))
}


# TODO
# - allow for an offset ?
# - allow for some covariates to be constrained to nonnegativity, use QP for this
# - make super function to perform nn.reg, nn.ridge, nn.adaptive.ridge
# - Explore standardization of y and X and inlfuence on lambda selection vs theoretical lambda


#' Set parameters for L0glm algorithm
#'
#' @description
#'
#' The functions generate a list with all required parameters for the control
#' the adaptive ridge iteration (\code{control.l0.gen}), the iteratively
#' weighted least squares for GLM fitting (\code{control.iwls.gen}), or the
#' least squares fitting (\code{control.fit.gen}).
#'
#' @param maxit
#' The maximum number of iterations.
#' @param rel.tol
#' Coefficients are update until the relative difference between two iteration
#' is smaller than this threshold. The relative difference is defined as
#' \eqn{2 \frac{|coef_i^{(k-1)} - coef_i^{(k)}|}{coef_i^{(k-1)} + coef_i^{(k)}}}
#' @param delta
#' A small constant added to the estimated coefficients when computing the
#' adaptive ridge penalty weights.
#' @param gamma
#' The exponent used for computing the adaptive ridge penalty weights. It is
#' recommended to work with \code{gamma = 2} (Frommlet et Nuel 2016).
#' @param warn
#' Should warnings from the iteration be produced ?
#' @param thresh
#' A threshold under which the fitted coefficients are clipped to 0.
#' @param block.size
#' Number of covariates to fit simultaneously. If \code{block.size} is smaller
#' than \code{p}, the fit is performed in different blocks which is updated
#' cyclically.
#' @param tol
#' Stop iteration when the difference in coefficients between iterations is
#' smaller than this threshold. The coefficient difference is defined as:
#' \eqn{\frac{\|coefs^{(k-1)} - coefs^{(k)}}{\|coefs^{(k-1)}\|}}
#'
#' @return
#'
#' The functions return a list with the required parameters for computing an
#' \code{L0glm} fit.
#'
#' @references
#'
#' Frommlet, F. and G., Nuel. (2016) An adaptive ridge procedure for L0 regularization. PloS one.
#'
#' @seealso
#'
#' \code{\link{L0glm}}
#'
#' @examples
#'
#' control.l0.gen()
#' control.iwls.gen()
#' control.fit.gen()
#'
#' @export
control.l0.gen <- function(maxit = 100, rel.tol = 1E-4, delta = 1E-5, gamma = 2, warn = FALSE){
  l <- list(maxit = maxit, rel.tol = rel.tol, delta = delta, gamma = gamma, warn = warn)
  return(l)
}


#' @describeIn control.l0.gen
#'
#' Generate parameters that control the IWLS iteration.
#'
#' @export
control.iwls.gen <- function(maxit = 100, rel.tol = 1E-4, thresh = 1E-5, warn = FALSE){
  l <- list(maxit = maxit, rel.tol = rel.tol, thresh = thresh, warn = warn)
  return(l)
}


#' @describeIn control.l0.gen
#'
#' Generates parameters that control the least square fit.
#'
#' @export
control.fit.gen <- function(maxit = 10, block.size = NULL, tol = 1E-7){
  l <- list(block.size = block.size, maxit = maxit, tol = tol)
  return(l)
}
